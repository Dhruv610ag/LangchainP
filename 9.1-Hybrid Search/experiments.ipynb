{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Search Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet  pinecone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key=\"pcsk_2odQAg_2Uaw8rJ5zvtrx1xSUi88TX9ECSKGZxyEgdgfWFAsWHGZu2auYy9PyAqPUy8h9gH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import PineconeHybridSearchRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhruv/Desktop/GenAI Projects/Genai/lib/python3.10/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "index_name=\"hybrid-search-langchain-pinecone\"\n",
    "## initialize the Pinecone client\n",
    "pc=Pinecone(api_key=api_key)\n",
    "\n",
    "#create the index\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,  # dimensionality of dense model this 384 is using because we are going to use the hugging face embedding techniques which also convert into the vector embedding of 384 \n",
    "        metric=\"dotproduct\",  # sparse values supported only for dotproduct\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pinecone.data.index.Index at 0x7fc12fccf190>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index=pc.Index(index_name)\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/Users/dhruv/Desktop/GenAI Projects/Genai/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Users/dhruv/Desktop/GenAI Projects/Genai/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/dhruv/Desktop/GenAI Projects/Genai/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/dhruv/Desktop/GenAI Projects/Genai/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/dhruv/Desktop/GenAI Projects/Genai/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/dhruv/Desktop/GenAI Projects/Genai/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Users/dhruv/Desktop/GenAI Projects/Genai/lib/python3.10/asyncio/base_events.py\", line 595, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Users/dhruv/Desktop/GenAI Projects/Genai/lib/python3.10/asyncio/base_events.py\", line 1881, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Users/dhruv/Desktop/GenAI Projects/Genai/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/dhruv/Desktop/GenAI Projects/Genai/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/dhruv/Desktop/GenAI Projects/Genai/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/dhruv/Desktop/GenAI Projects/Genai/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/dhruv/Desktop/GenAI Projects/Genai/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/dhruv/Desktop/GenAI Projects/Genai/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/dhruv/Desktop/GenAI Projects/Genai/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/dhruv/Desktop/GenAI Projects/Genai/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/dhruv/Desktop/GenAI Projects/Genai/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/dhruv/Desktop/GenAI Projects/Genai/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/dhruv/Desktop/GenAI Projects/Genai/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/dhruv/Desktop/GenAI Projects/Genai/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/dhruv/Desktop/GenAI Projects/Genai/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/dhruv/Desktop/GenAI Projects/Genai/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/z1/33mc3s7n3qj8rx1ly2trwtvc0000gn/T/ipykernel_7986/3034851775.py\", line 11, in <module>\n",
      "    embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
      "  File \"/Users/dhruv/Desktop/GenAI Projects/Genai/lib/python3.10/site-packages/langchain_huggingface/embeddings/huggingface.py\", line 69, in __init__\n",
      "    import sentence_transformers  # type: ignore[import]\n",
      "  File \"/Users/dhruv/Desktop/GenAI Projects/Genai/lib/python3.10/site-packages/sentence_transformers/__init__.py\", line 10, in <module>\n",
      "    from sentence_transformers.backend import (\n",
      "  File \"/Users/dhruv/Desktop/GenAI Projects/Genai/lib/python3.10/site-packages/sentence_transformers/backend/__init__.py\", line 3, in <module>\n",
      "    from .load import load_onnx_model, load_openvino_model\n",
      "  File \"/Users/dhruv/Desktop/GenAI Projects/Genai/lib/python3.10/site-packages/sentence_transformers/backend/load.py\", line 7, in <module>\n",
      "    from transformers.configuration_utils import PretrainedConfig\n",
      "  File \"/Users/dhruv/Desktop/GenAI Projects/Genai/lib/python3.10/site-packages/transformers/__init__.py\", line 27, in <module>\n",
      "    from . import dependency_versions_check\n",
      "  File \"/Users/dhruv/Desktop/GenAI Projects/Genai/lib/python3.10/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
      "    from .utils.versions import require_version, require_version_core\n",
      "  File \"/Users/dhruv/Desktop/GenAI Projects/Genai/lib/python3.10/site-packages/transformers/utils/__init__.py\", line 24, in <module>\n",
      "    from .auto_docstring import (\n",
      "  File \"/Users/dhruv/Desktop/GenAI Projects/Genai/lib/python3.10/site-packages/transformers/utils/auto_docstring.py\", line 30, in <module>\n",
      "    from .generic import ModelOutput\n",
      "  File \"/Users/dhruv/Desktop/GenAI Projects/Genai/lib/python3.10/site-packages/transformers/utils/generic.py\", line 53, in <module>\n",
      "    import torch  # noqa: F401\n",
      "  File \"/Users/dhruv/Desktop/GenAI Projects/Genai/lib/python3.10/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/dhruv/Desktop/GenAI Projects/Genai/lib/python3.10/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/dhruv/Desktop/GenAI Projects/Genai/lib/python3.10/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/dhruv/Desktop/GenAI Projects/Genai/lib/python3.10/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/dhruv/Desktop/GenAI Projects/Genai/lib/python3.10/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "## vector embedding and sparse matrix\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import warnings \n",
    "load_dotenv()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.environ[\"HF_TOKEN\"]=os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pinecone_text.sparse.bm25_encoder.BM25Encoder at 0x20f5d78bd30>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pinecone_text.sparse import BM25Encoder\n",
    "\n",
    "bm25_encoder=BM25Encoder().default()\n",
    "bm25_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 2995.93it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences=[\n",
    "    \"In 2023, I visited Paris\",\n",
    "        \"In 2022, I visited New York\",\n",
    "        \"In 2021, I visited New Orleans\",\n",
    "\n",
    "]\n",
    "\n",
    "## tfidf values on these sentence\n",
    "bm25_encoder.fit(sentences)\n",
    "\n",
    "## store the values to a json file\n",
    "bm25_encoder.dump(\"bm25_values.json\")\n",
    "\n",
    "# load to your BM25Encoder object\n",
    "bm25_encoder = BM25Encoder().load(\"bm25_values.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=PineconeHybridSearchRetriever(embeddings=embeddings,sparse_encoder=bm25_encoder,index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PineconeHybridSearchRetriever(embeddings=HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       "), model_name='all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False), sparse_encoder=<pinecone_text.sparse.bm25_encoder.BM25Encoder object at 0x0000020F55236530>, index=<pinecone.data.index.Index object at 0x0000020F5D2F4730>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.54s/it]\n"
     ]
    }
   ],
   "source": [
    "retriever.add_texts(\n",
    "    [\n",
    "    \"In 2023, I visited Paris\",\n",
    "        \"In 2022, I visited New York\",\n",
    "        \"In 2021, I visited New Orleans\",\n",
    "\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='In 2021, I visited New Orleans'),\n",
       " Document(page_content='In 2022, I visited New York'),\n",
       " Document(page_content='In 2023, I visited Paris')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"What city did i visit first\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
